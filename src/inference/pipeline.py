"""
Inference Pipeline

Production-ready inference wrapper for the multi-task sentiment model.

Features:
    - Single-sample and batch prediction
    - Preprocessing integration (LID + normalization)
    - Probability calibration (optional)
    - Output formatting with confidence scores
    - Device management (CPU/CUDA/MPS)
    - Model loading from checkpoint

Usage:
    pipeline = InferencePipeline.from_checkpoint("checkpoints/best_model.pt")
    result = pipeline.predict("This product is AMAZING!!! ğŸ˜")
    # â†’ {"sentiment": "positive", "sentiment_probs": [...], "sarcasm": False, ...}
"""

from pathlib import Path
from typing import Dict, List, Optional, Union

import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    import torch
    import torch.nn as nn
    from transformers import AutoTokenizer
except ImportError:
    torch = None
    nn = None

from utils.logger import get_logger
from utils.config_loader import load_config
from preprocessing.language_identifier import TokenLevelLID
from preprocessing.text_normalizer import TextNormalizer
from models.sarcasm_features import SarcasmSignalExtractor

logger = get_logger("inference")


class InferencePipeline:
    """
    End-to-end inference pipeline.
    
    Pipeline stages:
        1. Text normalization (preserve sentiment signals)
        2. Language identification per token
        3. Tokenization (XLM-RoBERTa)
        4. Model forward pass
        5. Probability calibration (if calibrator provided)
        6. Output formatting
    
    Args:
        model:       Trained MultiTaskSentimentModel
        tokenizer:   HuggingFace tokenizer
        normalizer:  TextNormalizer instance
        lid:         TokenLevelLID instance
        calibrator:  Optional MultiTaskCalibrator for temperature scaling
        device:      'cpu' | 'cuda' | 'mps' | None (auto-detect)
        max_length:  Maximum sequence length
    """
    
    def __init__(
        self,
        model: "nn.Module",
        tokenizer,
        normalizer: TextNormalizer,
        lid: TokenLevelLID,
        sarcasm_extractor: SarcasmSignalExtractor,
        calibrator=None,
        device: Optional[str] = None,
        max_length: int = 128,
    ):
        if torch is None:
            raise RuntimeError("torch is required for inference")
        
        self.model = model
        self.tokenizer = tokenizer
        self.normalizer = normalizer
        self.lid = lid
        self.sarcasm_extractor = sarcasm_extractor
        self.calibrator = calibrator
        self.max_length = max_length
        
        # Device selection
        if device is None:
            if torch.cuda.is_available():
                device = "cuda"
            elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                device = "mps"
            else:
                device = "cpu"
        
        self.device = torch.device(device)
        self.model = self.model.to(self.device)
        self.model.eval()
        
        # Label mappings
        self.sentiment_labels = ["negative", "neutral", "positive"]
        self.sarcasm_labels = ["literal", "sarcastic"]
        
        logger.info(f"InferencePipeline ready | device={device}")
    
    def preprocess(
        self,
        text: str,
    ) -> Dict[str, "torch.Tensor"]:
        """
        Preprocess a single text sample.
        
        Args:
            text: Raw input string
            
        Returns:
            Dict with input_ids, attention_mask, lid_token_ids, aux_features
        """
        # 1. Normalize text (preserve sentiment signals)
        normalized = self.normalizer.normalize(text)
        
        # 2. Tokenize
        encoding = self.tokenizer(
            normalized,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )
        
        # 3. Language ID per token (simplified for inference)
        lid_result = self.lid.identify_sentence(normalized)
        lid_ids = [0] * self.max_length  # Default to 'en'
        
        lid_tensor = torch.tensor([lid_ids], dtype=torch.long)
        
        # 4. Sarcasm auxiliary features (from raw text)
        aux_features = self.sarcasm_extractor.extract(text)
        aux_tensor = torch.tensor(
            [[aux_features["punctuation_intensity"],
              aux_features["allcaps_ratio"],
              aux_features["elongation_count"],
              aux_features["emoji_sentiment"]]],
            dtype=torch.float32
        )
        
        return {
            "input_ids":      encoding["input_ids"],
            "attention_mask": encoding["attention_mask"],
            "lid_token_ids":  lid_tensor,
            "aux_features":   aux_tensor,
        }
    
    @torch.no_grad()
    def predict_single(
        self,
        text: str,
        return_probabilities: bool = True,
        apply_calibration: bool = True,
    ) -> Dict:
        """
        Predict sentiment and sarcasm for a single text.
        
        Args:
            text:                  Input string
            return_probabilities:  Include class probabilities
            apply_calibration:     Use temperature scaling if available
            
        Returns:
            Dict with sentiment, sarcasm, confidence scores, etc.
        """
        # Preprocess
        inputs = self.preprocess(text)
        
        # Move to device
        input_ids      = inputs["input_ids"].to(self.device)
        attention_mask = inputs["attention_mask"].to(self.device)
        lid_token_ids  = inputs["lid_token_ids"].to(self.device)
        
        # Forward pass
        outputs = self.model(input_ids, attention_mask, lid_token_ids)
        
        sentiment_logits = outputs["sentiment_logits"]  # (1, 3)
        sarcasm_logit    = outputs["sarcasm_logit"]     # (1, 1)
        
        # Apply calibration if available
        if apply_calibration and self.calibrator is not None:
            sentiment_probs = self.calibrator.calibrate_sentiment(
                sentiment_logits
            ).cpu().numpy()[0]
            sarcasm_prob = self.calibrator.calibrate_sarcasm(
                sarcasm_logit
            ).cpu().item()
        else:
            sentiment_probs = torch.softmax(sentiment_logits, dim=-1).cpu().numpy()[0]
            sarcasm_prob = torch.sigmoid(sarcasm_logit).cpu().item()
        
        # Predictions
        sentiment_idx = sentiment_probs.argmax()
        sarcasm_pred = sarcasm_prob >= 0.5
        
        result = {
            "text": text,
            "sentiment": self.sentiment_labels[sentiment_idx],
            "sentiment_confidence": float(sentiment_probs[sentiment_idx]),
            "sarcasm": self.sarcasm_labels[1] if sarcasm_pred else self.sarcasm_labels[0],
            "sarcasm_confidence": float(sarcasm_prob if sarcasm_pred else 1 - sarcasm_prob),
        }
        
        if return_probabilities:
            result["sentiment_probs"] = {
                label: float(prob)
                for label, prob in zip(self.sentiment_labels, sentiment_probs)
            }
            result["sarcasm_prob"] = float(sarcasm_prob)
        
        return result
    
    def predict_batch(
        self,
        texts: List[str],
        batch_size: int = 32,
        **kwargs
    ) -> List[Dict]:
        """
        Batch prediction for multiple texts.
        
        Args:
            texts:      List of input strings
            batch_size: Batch size for processing
            **kwargs:   Passed to predict_single
            
        Returns:
            List of prediction dicts
        """
        results = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            for text in batch:
                results.append(self.predict_single(text, **kwargs))
        
        return results
    
    @classmethod
    def from_checkpoint(
        cls,
        checkpoint_path: str,
        config_dir: str = "config",
        device: Optional[str] = None,
    ) -> "InferencePipeline":
        """
        Load pipeline from saved checkpoint.
        
        Args:
            checkpoint_path: Path to model checkpoint (.pt file)
            config_dir:      Directory containing config YAMLs
            device:          Target device
            
        Returns:
            Initialized InferencePipeline
        """
        if torch is None:
            raise RuntimeError("torch is required")
        
        # Load configs
        model_config = load_config(f"{config_dir}/model_config.yaml")
        preproc_config = load_config(f"{config_dir}/preprocessing_config.yaml")
        
        # Load model architecture
        from models.multitask_transformer import MultiTaskSentimentModel
        model = MultiTaskSentimentModel.from_config(model_config)
        
        # Load weights
        checkpoint = torch.load(checkpoint_path, map_location="cpu")
        if "model_state" in checkpoint:
            model.load_state_dict(checkpoint["model_state"])
        else:
            model.load_state_dict(checkpoint)
        
        model.eval()
        
        # Load tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            model_config["model"]["base_model"]
        )
        
        # Initialize preprocessing
        normalizer = TextNormalizer.from_config(preproc_config)
        lid = TokenLevelLID()
        sarcasm_extractor = SarcasmSignalExtractor()
        
        logger.info(f"Pipeline loaded from {checkpoint_path}")
        
        return cls(
            model=model,
            tokenizer=tokenizer,
            normalizer=normalizer,
            lid=lid,
            sarcasm_extractor=sarcasm_extractor,
            device=device,
            max_length=preproc_config["preprocessing"]["tokenization"]["max_length"],
        )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Smoke-test
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    if torch is None:
        print("âš  torch not available â€” skipping inference test")
    else:
        print("InferencePipeline smoke-test\n")
        print("Note: Requires trained model checkpoint to run full test.")
        print("This test validates the API structure only.\n")
        
        # Mock pipeline components for testing
        from preprocessing.text_normalizer import TextNormalizer
        from preprocessing.language_identifier import TokenLevelLID
        from models.sarcasm_features import SarcasmSignalExtractor
        
        normalizer = TextNormalizer.default()
        lid = TokenLevelLID()
        sarcasm_extractor = SarcasmSignalExtractor()
        
        # Test preprocessing
        text = "This product is AMAZING!!! ğŸ˜"
        normalized = normalizer.normalize(text)
        print(f"Original: {text}")
        print(f"Normalized: {normalized}")
        
        aux = sarcasm_extractor.extract(text)
        print(f"Sarcasm features: {aux}")
        
        print("\nâœ“ InferencePipeline structure validated!")
